"""
ë°ì´í„° ì „ì²˜ë¦¬ ëª¨ë“ˆ (Data Preprocessing Module)
- ì‹œê³„ì—´ ë°ì´í„° ë¡œë“œ, í•„í„°ë§, ì´ìƒê°’ ì²˜ë¦¬, ì •ê·œí™”
- Date í˜•ì‹ ë³´ì¡´ ë° ì‹œê°„ ì¬ì •ë ¬ ê¸°ëŠ¥
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Any


class DataPreprocessor:
    """ì‹œê³„ì—´ ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    # ìš©ì–´ ë„ì›€ë§
    HELP_TEXTS = {
        '2sigma': '2Ïƒ (2 í‘œì¤€í¸ì°¨): í‰ê· ì—ì„œ Â±2 í‘œì¤€í¸ì°¨ ë²”ìœ„. ì •ê·œë¶„í¬ ê¸°ì¤€ ì•½ 95.4%ì˜ ë°ì´í„° í¬í•¨. ì—„ê²©í•œ í•„í„°ë§ì— ì í•©.',
        '2.5sigma': '2.5Ïƒ (2.5 í‘œì¤€í¸ì°¨): í‰ê· ì—ì„œ Â±2.5 í‘œì¤€í¸ì°¨ ë²”ìœ„. ì •ê·œë¶„í¬ ê¸°ì¤€ ì•½ 98.8%ì˜ ë°ì´í„° í¬í•¨. [ê¶Œì¥]',
        '3sigma': '3Ïƒ (3 í‘œì¤€í¸ì°¨): í‰ê· ì—ì„œ Â±3 í‘œì¤€í¸ì°¨ ë²”ìœ„. ì •ê·œë¶„í¬ ê¸°ì¤€ ì•½ 99.7%ì˜ ë°ì´í„° í¬í•¨. ëŠìŠ¨í•œ í•„í„°ë§ì— ì í•©.',
        'iqr': 'IQR (ì‚¬ë¶„ìœ„ ë²”ìœ„): Q1-1.5Ã—IQR ~ Q3+1.5Ã—IQR ë²”ìœ„. ë¹„ëŒ€ì¹­ ë¶„í¬ì— ì í•©. ê·¹ë‹¨ì  ì´ìƒê°’ íƒì§€ì— íš¨ê³¼ì .',
        'zscore': 'Z-Score ì •ê·œí™”: (ê°’ - í‰ê· ) / í‘œì¤€í¸ì°¨. í‰ê· =0, í‘œì¤€í¸ì°¨=1ë¡œ ë³€í™˜. ë°ì´í„° ë¹„êµ ì‹œ ìœ ìš©.',
        'minmax': 'Min-Max ì •ê·œí™”: (ê°’ - ìµœì†Œ) / (ìµœëŒ€ - ìµœì†Œ). 0~1 ë²”ìœ„ë¡œ ë³€í™˜. ì‹ ê²½ë§ ì…ë ¥ì— ì í•©.'
    }
    
    def __init__(self):
        self.original_df: Optional[pd.DataFrame] = None
        self.processed_df: Optional[pd.DataFrame] = None
        self.columns: List[str] = []
        self.numeric_columns: List[str] = []
        self.date_column: Optional[str] = None
        self.original_date_format: Optional[str] = None  # ì›ë³¸ ë‚ ì§œ í˜•ì‹ ì €ì¥
        self.stats: Dict[str, Any] = {}
    
    def load_data(self, file_path: str) -> Tuple[bool, str]:
        """
        Excel ë˜ëŠ” CSV íŒŒì¼ì„ ë¡œë“œí•˜ê³  ì»¬ëŸ¼ì„ ìë™ ê°ì§€í•©ë‹ˆë‹¤.
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            
        Returns:
            (ì„±ê³µ ì—¬ë¶€, ë©”ì‹œì§€)
        """
        try:
            path = Path(file_path)
            
            if path.suffix.lower() in ['.xlsx', '.xls']:
                self.original_df = pd.read_excel(file_path)
            elif path.suffix.lower() == '.csv':
                # ì¸ì½”ë”© ìë™ ê°ì§€ ì‹œë„
                try:
                    self.original_df = pd.read_csv(file_path, encoding='utf-8')
                except UnicodeDecodeError:
                    try:
                        self.original_df = pd.read_csv(file_path, encoding='cp949')
                    except UnicodeDecodeError:
                        self.original_df = pd.read_csv(file_path, encoding='euc-kr')
            else:
                return False, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {path.suffix}"
            
            self.processed_df = self.original_df.copy()
            self.columns = list(self.original_df.columns)
            
            # ë‚ ì§œ ì»¬ëŸ¼ ìë™ ê°ì§€ (í˜•ì‹ ë³´ì¡´)
            self._detect_date_column()
            
            # ìˆ«ì ì»¬ëŸ¼ ê°ì§€ (ìµœëŒ€ 30ê°œ)
            self._detect_numeric_columns()
            
            self.stats['original_rows'] = len(self.original_df)
            self.stats['columns'] = len(self.columns)
            self.stats['numeric_columns'] = len(self.numeric_columns)
            
            return True, f"íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(self.original_df)}í–‰, {len(self.columns)}ì—´"
            
        except Exception as e:
            return False, f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
    
    def _detect_date_column(self):
        """ë‚ ì§œ ì»¬ëŸ¼ì„ ìë™ ê°ì§€í•©ë‹ˆë‹¤. ì›ë³¸ í˜•ì‹ì„ ë³´ì¡´í•©ë‹ˆë‹¤."""
        date_keywords = ['date', 'time', 'datetime', 'ë‚ ì§œ', 'ì‹œê°„', 'timestamp']
        
        for col in self.columns:
            if any(keyword in col.lower() for keyword in date_keywords):
                self.date_column = col
                
                # ì›ë³¸ í˜•ì‹ ìƒ˜í”Œ ì €ì¥ (ì²« ë²ˆì§¸ ìœ íš¨í•œ ê°’)
                sample_value = self.original_df[col].dropna().iloc[0] if len(self.original_df[col].dropna()) > 0 else None
                if sample_value is not None:
                    self.original_date_format = str(sample_value)
                
                # ë‚ ì§œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ì‹œë„ (ë‚´ë¶€ ì²˜ë¦¬ìš©)
                try:
                    self.original_df[col] = pd.to_datetime(self.original_df[col])
                    self.processed_df[col] = pd.to_datetime(self.processed_df[col])
                except:
                    pass
                break
    
    def _detect_numeric_columns(self):
        """ìˆ«ì ì»¬ëŸ¼ì„ ê°ì§€í•©ë‹ˆë‹¤. ìµœëŒ€ 30ê°œê¹Œì§€ ì§€ì›."""
        self.numeric_columns = []
        for col in self.columns:
            if col != self.date_column:
                if pd.api.types.is_numeric_dtype(self.original_df[col]):
                    self.numeric_columns.append(col)
                    if len(self.numeric_columns) >= 30:  # ìµœëŒ€ 30ê°œ
                        break
    
    def get_column_stats(self, column: str) -> Dict[str, float]:
        """íŠ¹ì • ì»¬ëŸ¼ì˜ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if column not in self.numeric_columns:
            return {}
        
        data = self.processed_df[column].dropna()
        
        return {
            'count': len(data),
            'mean': data.mean(),
            'std': data.std(),
            'min': data.min(),
            'max': data.max(),
            'q1': data.quantile(0.25),
            'median': data.median(),
            'q3': data.quantile(0.75)
        }
    
    @classmethod
    def get_help_text(cls, key: str) -> str:
        """ìš©ì–´ì— ëŒ€í•œ ë„ì›€ë§ ë°˜í™˜"""
        return cls.HELP_TEXTS.get(key, "ë„ì›€ë§ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    def apply_filters(self, filters: List[Dict]) -> Tuple[bool, str]:
        """
        ë‹¤ì¤‘ ì¡°ê±´ìœ¼ë¡œ ë°ì´í„°ë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤ (AND ì¡°ê±´).
        
        Args:
            filters: í•„í„° ì¡°ê±´ ëª©ë¡
                [
                    {'column': 'AMBIENT_TEMP', 'operator': '>=', 'value': 15},
                    {'column': 'FAN_CURRENT', 'operator': 'range', 'min': 30, 'max': 50}
                ]
        
        Returns:
            (ì„±ê³µ ì—¬ë¶€, ë©”ì‹œì§€)
        """
        try:
            if self.original_df is None:
                return False, "ë¨¼ì € ë°ì´í„°ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”."
            
            # ì›ë³¸ì—ì„œ ë‹¤ì‹œ ì‹œì‘
            self.processed_df = self.original_df.copy()
            
            before_count = len(self.processed_df)
            
            mask = pd.Series([True] * len(self.processed_df))
            
            for f in filters:
                column = f.get('column')
                operator = f.get('operator')
                
                if column not in self.columns:
                    continue
                
                col_data = self.processed_df[column]
                
                if operator == '>=':
                    mask &= col_data >= f.get('value', 0)
                elif operator == '<=':
                    mask &= col_data <= f.get('value', 0)
                elif operator == '>':
                    mask &= col_data > f.get('value', 0)
                elif operator == '<':
                    mask &= col_data < f.get('value', 0)
                elif operator == '=':
                    mask &= col_data == f.get('value', 0)
                elif operator == '!=':
                    mask &= col_data != f.get('value', 0)
                elif operator == 'range':
                    min_val = f.get('min', float('-inf'))
                    max_val = f.get('max', float('inf'))
                    mask &= (col_data >= min_val) & (col_data <= max_val)
            
            self.processed_df = self.processed_df[mask].reset_index(drop=True)
            
            after_count = len(self.processed_df)
            self.stats['filtered_rows'] = after_count
            self.stats['filter_removed'] = before_count - after_count
            
            return True, f"í•„í„°ë§ ì™„ë£Œ: {before_count} â†’ {after_count}í–‰ ({after_count/before_count*100:.1f}%)"
            
        except Exception as e:
            return False, f"í•„í„°ë§ ì‹¤íŒ¨: {str(e)}"
    
    def remove_outliers(self, 
                       method: str = '2.5sigma',
                       columns: Optional[List[str]] = None,
                       action: str = 'drop') -> Tuple[bool, str]:
        """
        ì´ìƒê°’ì„ ì œê±°í•©ë‹ˆë‹¤.
        
        Args:
            method: ì´ìƒê°’ íƒì§€ ë°©ë²•
                - '2sigma': Â±2 í‘œì¤€í¸ì°¨ (95.4% í¬í•¨)
                - '2.5sigma': Â±2.5 í‘œì¤€í¸ì°¨ (98.8% í¬í•¨) [ê¶Œì¥]
                - '3sigma': Â±3 í‘œì¤€í¸ì°¨ (99.7% í¬í•¨)
                - 'iqr': IQR ë°©ì‹
            columns: ì ìš©í•  ì»¬ëŸ¼ ëª©ë¡ (Noneì´ë©´ ëª¨ë“  ìˆ«ì ì»¬ëŸ¼)
            action: ì´ìƒê°’ ì²˜ë¦¬ ë°©ë²•
                - 'nan': í•´ë‹¹ ê°’ë§Œ NaNìœ¼ë¡œ ë³€ê²½
                - 'drop': í•´ë‹¹ í–‰ ì „ì²´ ì‚­ì œ [ê¸°ë³¸ê°’]
        
        Returns:
            (ì„±ê³µ ì—¬ë¶€, ë©”ì‹œì§€)
        """
        try:
            if self.processed_df is None:
                return False, "ë¨¼ì € ë°ì´í„°ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”."
            
            target_columns = columns if columns else self.numeric_columns
            outlier_count = 0
            
            for col in target_columns:
                if col not in self.numeric_columns:
                    continue
                
                data = self.processed_df[col]
                
                if method == 'iqr':
                    q1 = data.quantile(0.25)
                    q3 = data.quantile(0.75)
                    iqr = q3 - q1
                    lower = q1 - 1.5 * iqr
                    upper = q3 + 1.5 * iqr
                else:
                    # í‘œì¤€í¸ì°¨ ê¸°ë°˜
                    sigma_map = {
                        '2sigma': 2.0,
                        '2.5sigma': 2.5,
                        '3sigma': 3.0
                    }
                    n = sigma_map.get(method, 2.5)
                    
                    mean = data.mean()
                    std = data.std()
                    lower = mean - n * std
                    upper = mean + n * std
                
                # ì´ìƒê°’ ë§ˆìŠ¤í¬
                outlier_mask = (data < lower) | (data > upper)
                col_outliers = outlier_mask.sum()
                outlier_count += col_outliers
                
                if action == 'nan':
                    self.processed_df.loc[outlier_mask, col] = np.nan
                elif action == 'drop':
                    self.processed_df = self.processed_df[~outlier_mask]
            
            if action == 'drop':
                self.processed_df = self.processed_df.reset_index(drop=True)
            
            self.stats['outliers_removed'] = outlier_count
            self.stats['rows_after_outlier'] = len(self.processed_df)
            
            method_names = {
                '2sigma': '2Ïƒ (95.4%)',
                '2.5sigma': '2.5Ïƒ (98.8%)',
                '3sigma': '3Ïƒ (99.7%)',
                'iqr': 'IQR'
            }
            
            return True, f"ì´ìƒê°’ ì²˜ë¦¬ ì™„ë£Œ ({method_names.get(method, method)}): {outlier_count}ê°œ ì²˜ë¦¬"
            
        except Exception as e:
            return False, f"ì´ìƒê°’ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
    
    def normalize_data(self, 
                      method: str = 'zscore',
                      columns: Optional[List[str]] = None) -> Tuple[bool, str]:
        """
        ë°ì´í„°ë¥¼ ì •ê·œí™”í•©ë‹ˆë‹¤.
        
        Args:
            method: ì •ê·œí™” ë°©ë²•
                - 'zscore': Z-Score ì •ê·œí™” (x - Î¼) / Ïƒ
                - 'minmax': Min-Max ì •ê·œí™” (0~1 ë²”ìœ„)
            columns: ì ìš©í•  ì»¬ëŸ¼ ëª©ë¡ (Noneì´ë©´ ëª¨ë“  ìˆ«ì ì»¬ëŸ¼)
        
        Returns:
            (ì„±ê³µ ì—¬ë¶€, ë©”ì‹œì§€)
        """
        try:
            if self.processed_df is None:
                return False, "ë¨¼ì € ë°ì´í„°ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”."
            
            target_columns = columns if columns else self.numeric_columns
            normalized_count = 0
            
            for col in target_columns:
                if col not in self.numeric_columns:
                    continue
                
                data = self.processed_df[col]
                
                if method == 'zscore':
                    mean = data.mean()
                    std = data.std()
                    if std != 0:
                        self.processed_df[col] = (data - mean) / std
                        normalized_count += 1
                        
                elif method == 'minmax':
                    min_val = data.min()
                    max_val = data.max()
                    if max_val != min_val:
                        self.processed_df[col] = (data - min_val) / (max_val - min_val)
                        normalized_count += 1
            
            method_names = {
                'zscore': 'Z-Score',
                'minmax': 'Min-Max'
            }
            
            return True, f"ì •ê·œí™” ì™„ë£Œ ({method_names.get(method, method)}): {normalized_count}ê°œ ì»¬ëŸ¼"
            
        except Exception as e:
            return False, f"ì •ê·œí™” ì‹¤íŒ¨: {str(e)}"
    
    def normalize_timestamps(self, interval_minutes: int = 2) -> Tuple[bool, str]:
        """
        ì‹œê°„ì„ ê°€ì¥ ê°€ê¹Œìš´ ì§€ì •ëœ ê°„ê²©ìœ¼ë¡œ ì •ê·œí™”(ìŠ¤ëƒ…)í•©ë‹ˆë‹¤.
        ì˜ˆ: 00:01:00 â†’ 00:00:00, 00:02:01 â†’ 00:02:00, 00:05:59 â†’ 00:06:00
        
        ì—‘ì…€ ìë™ì±„ìš°ê¸°ì—ì„œ ë°œìƒí•˜ëŠ” ì‹œê°„ ë°€ë¦¼ í˜„ìƒì„ ë³´ì •í•©ë‹ˆë‹¤.
        
        Args:
            interval_minutes: ê°„ê²© (ë¶„), ê¸°ë³¸ê°’ 2ë¶„
        
        Returns:
            (ì„±ê³µ ì—¬ë¶€, ë©”ì‹œì§€)
        """
        try:
            if self.processed_df is None or self.date_column is None:
                return False, "ë°ì´í„° ë˜ëŠ” ë‚ ì§œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤."
            
            # ë‚ ì§œ ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜
            dates = pd.to_datetime(self.processed_df[self.date_column])
            
            corrected_count = 0
            new_times = []
            
            for dt in dates:
                # ì›ë³¸ ì‹œê°„ì˜ ì´ ë¶„ ê³„ì‚° (ì´ˆ í¬í•¨)
                total_minutes = dt.hour * 60 + dt.minute + dt.second / 60 + dt.microsecond / 60000000
                
                # ê°€ì¥ ê°€ê¹Œìš´ ê°„ê²©ìœ¼ë¡œ ë°˜ì˜¬ë¦¼
                snapped_minutes = round(total_minutes / interval_minutes) * interval_minutes
                
                # 24ì‹œê°„ ë„˜ì–´ê°€ë©´ ë‹¤ìŒ ë‚ ë¡œ
                days_add = int(snapped_minutes // (24 * 60))
                snapped_minutes = snapped_minutes % (24 * 60)
                
                snapped_hour = int(snapped_minutes // 60)
                snapped_min = int(snapped_minutes % 60)
                
                # ìƒˆ ì‹œê°„ ìƒì„±
                try:
                    new_dt = dt.replace(hour=snapped_hour, minute=snapped_min, second=0, microsecond=0)
                    if days_add > 0:
                        new_dt = new_dt + timedelta(days=days_add)
                except:
                    new_dt = dt
                
                # ë³€ê²½ ì—¬ë¶€ í™•ì¸
                if dt.minute != snapped_min or dt.second != 0 or dt.microsecond != 0:
                    corrected_count += 1
                
                new_times.append(new_dt)
            
            # ë‚ ì§œ ì»¬ëŸ¼ ì—…ë°ì´íŠ¸
            self.processed_df[self.date_column] = new_times
            
            return True, f"ì‹œê°„ ì •ê·œí™” ì™„ë£Œ: {corrected_count}ê°œ ì‹œê°„ ë³´ì • ({interval_minutes}ë¶„ ê°„ê²©)"
            
        except Exception as e:
            return False, f"ì‹œê°„ ì •ê·œí™” ì‹¤íŒ¨: {str(e)}"
    
    def realign_timestamps(self, 
                          start_time: str,
                          interval_minutes: int = 2) -> Tuple[bool, str]:
        """
        ì‹œê°„ì„ ì¬ì •ë ¬í•©ë‹ˆë‹¤. ì§€ì •ëœ ì‹œì‘ ì‹œê°„ë¶€í„° ì¼ì • ê°„ê²©ìœ¼ë¡œ ì¬ë°°ì—´.
        
        Args:
            start_time: ì‹œì‘ ì‹œê°„ (yyyy-mm-dd hh:mm:ss í˜•ì‹)
            interval_minutes: ê°„ê²© (ë¶„), ê¸°ë³¸ê°’ 2ë¶„
        
        Returns:
            (ì„±ê³µ ì—¬ë¶€, ë©”ì‹œì§€)
        """
        try:
            if self.processed_df is None or self.date_column is None:
                return False, "ë°ì´í„° ë˜ëŠ” ë‚ ì§œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤."
            
            # ì‹œì‘ ì‹œê°„ íŒŒì‹±
            start_dt = pd.to_datetime(start_time)
            
            # ìƒˆë¡œìš´ ì‹œê°„ ìƒì„±
            num_rows = len(self.processed_df)
            new_times = [start_dt + timedelta(minutes=interval_minutes * i) for i in range(num_rows)]
            
            # ë‚ ì§œ ì»¬ëŸ¼ ì—…ë°ì´íŠ¸
            self.processed_df[self.date_column] = new_times
            
            return True, f"ì‹œê°„ ì¬ì •ë ¬ ì™„ë£Œ: {start_time}ë¶€í„° {interval_minutes}ë¶„ ê°„ê²©, {num_rows}í–‰"
            
        except Exception as e:
            return False, f"ì‹œê°„ ì¬ì •ë ¬ ì‹¤íŒ¨: {str(e)}"
    
    def save_data(self, 
                 output_path: Optional[str] = None,
                 original_path: Optional[str] = None,
                 date_format: str = '%Y-%m-%d %H:%M:%S') -> Tuple[bool, str]:
        """
        ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤. ë‚ ì§œ í˜•ì‹ì„ ë³´ì¡´í•©ë‹ˆë‹¤.
        
        Args:
            output_path: ì €ì¥ ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)
            original_path: ì›ë³¸ íŒŒì¼ ê²½ë¡œ (íŒŒì¼ëª… ìƒì„±ìš©)
            date_format: ë‚ ì§œ ì €ì¥ í˜•ì‹
        
        Returns:
            (ì„±ê³µ ì—¬ë¶€, ë©”ì‹œì§€ ë˜ëŠ” ì €ì¥ ê²½ë¡œ)
        """
        try:
            if self.processed_df is None:
                return False, "ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
            
            # ì €ì¥ìš© ë³µì‚¬ë³¸ ìƒì„±
            save_df = self.processed_df.copy()
            
            # ë‚ ì§œ ì»¬ëŸ¼ í˜•ì‹ ë³€í™˜ (ì €ì¥ ì‹œ ë¬¸ìì—´ë¡œ)
            if self.date_column and self.date_column in save_df.columns:
                try:
                    save_df[self.date_column] = pd.to_datetime(save_df[self.date_column]).dt.strftime(date_format)
                except:
                    pass  # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ê·¸ëŒ€ë¡œ ì €ì¥
            
            if output_path is None:
                if original_path:
                    orig_path = Path(original_path)
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    output_path = orig_path.parent / f"{orig_path.stem}_processed_{timestamp}{orig_path.suffix}"
                else:
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    output_path = f"processed_data_{timestamp}.csv"
            
            output_path = Path(output_path)
            
            if output_path.suffix.lower() in ['.xlsx', '.xls']:
                save_df.to_excel(output_path, index=False)
            else:
                save_df.to_csv(output_path, index=False, encoding='utf-8-sig')
            
            return True, str(output_path)
            
        except Exception as e:
            return False, f"ì €ì¥ ì‹¤íŒ¨: {str(e)}"
    
    def get_preview(self, rows: int = 10) -> pd.DataFrame:
        """ë¯¸ë¦¬ë³´ê¸°ìš© ë°ì´í„° ë°˜í™˜ (ì‹¤ì œ ì»¬ëŸ¼ë§Œ)"""
        if self.processed_df is not None:
            return self.processed_df.head(rows)
        return pd.DataFrame()
    
    def get_summary(self) -> str:
        """ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ë¬¸ìì—´ ë°˜í™˜"""
        lines = []
        lines.append(f"ğŸ“Š ì „ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½")
        lines.append(f"{'â”€' * 40}")
        
        if 'original_rows' in self.stats:
            lines.append(f"ì›ë³¸ ë°ì´í„°: {self.stats['original_rows']:,}í–‰")
        
        if 'filtered_rows' in self.stats:
            removed = self.stats.get('filter_removed', 0)
            lines.append(f"í•„í„°ë§ í›„: {self.stats['filtered_rows']:,}í–‰ (-{removed:,})")
        
        if 'outliers_removed' in self.stats:
            lines.append(f"ì´ìƒê°’ ì²˜ë¦¬: {self.stats['outliers_removed']:,}ê°œ")
        
        if 'rows_after_outlier' in self.stats:
            lines.append(f"ìµœì¢… ë°ì´í„°: {self.stats['rows_after_outlier']:,}í–‰")
        
        return "\n".join(lines)


# í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„± í•¨ìˆ˜
def create_sample_data(output_path: str = "sample_data.csv"):
    """í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
    np.random.seed(42)
    n = 1000
    
    dates = pd.date_range(start='2025-11-27', periods=n, freq='h')
    
    data = {
        'Date': dates,
        'AMBIENT_TEMP': np.random.normal(20, 5, n),
        'FAN_CURRENT': np.random.normal(45, 10, n),
        'GEARBOX_OIL_TEMP': np.random.normal(65, 8, n),
        'CWP_INTK_PIT_TEMP': np.random.normal(30, 3, n),
        'CONDR_TEMP_RISE': np.random.normal(10, 2, n)
    }
    
    # ì´ìƒê°’ ì¶”ê°€
    data['FAN_CURRENT'][50] = 150  # ê·¹ë‹¨ì  ì´ìƒê°’
    data['FAN_CURRENT'][100] = -20
    data['AMBIENT_TEMP'][200] = 60
    
    df = pd.DataFrame(data)
    df.to_csv(output_path, index=False)
    print(f"ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ: {output_path}")
    return output_path


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    sample_path = create_sample_data()
    
    preprocessor = DataPreprocessor()
    
    # 1. ë°ì´í„° ë¡œë“œ
    success, msg = preprocessor.load_data(sample_path)
    print(msg)
    print(f"ê°ì§€ëœ ìˆ«ì ì»¬ëŸ¼: {preprocessor.numeric_columns}")
    
    # 2. í•„í„°ë§
    filters = [
        {'column': 'AMBIENT_TEMP', 'operator': '>=', 'value': 15},
        {'column': 'FAN_CURRENT', 'operator': 'range', 'min': 30, 'max': 60}
    ]
    success, msg = preprocessor.apply_filters(filters)
    print(msg)
    
    # 3. ì´ìƒê°’ ì œê±° (ê¸°ë³¸ê°’: í–‰ ì „ì²´ ì‚­ì œ)
    success, msg = preprocessor.remove_outliers(method='2.5sigma', action='drop')
    print(msg)
    
    # 4. ì €ì¥
    success, output = preprocessor.save_data(original_path=sample_path)
    print(f"ì €ì¥ ì™„ë£Œ: {output}")
    
    # 5. ìš”ì•½
    print("\n" + preprocessor.get_summary())
